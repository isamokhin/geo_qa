{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import re\n",
    "from collections import OrderedDict, defaultdict\n",
    "import pickle\n",
    "import string\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from ukr_stemmer3 import UkrainianStemmer\n",
    "from tokenize_uk import tokenize_words, tokenize_sents\n",
    "from perceptron_tagger.tagger import PerceptronTagger\n",
    "tagger = PerceptronTagger()\n",
    "from sklearn.externals import joblib\n",
    "from difflib import get_close_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragedPerceptron(object):\n",
    "    \"\"\"\n",
    "    A class for training a model for answering the questions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Each feature gets its own weight vector, so weights is a dict-of-dicts\n",
    "        self.weights = {}\n",
    "        self.classes = set()\n",
    "        # The accumulated values, for the averaging. These will be keyed by\n",
    "        # feature/clas tuples\n",
    "        self._totals = defaultdict(int)\n",
    "        # The last time the feature was changed, for the averaging. Also\n",
    "        # keyed by feature/clas tuples\n",
    "        # (tstamps is short for timestamps)\n",
    "        self._tstamps = defaultdict(int)\n",
    "        # Number of instances seen\n",
    "        self.i = 0\n",
    "\n",
    "    def get_scores(self, features):\n",
    "        '''Dot-product the features and current weights and return the best label.'''\n",
    "        scores = defaultdict(float)\n",
    "        for feat, value in features.items():\n",
    "            if isinstance(value, list):\n",
    "                wordlist = value\n",
    "                for i, w in enumerate(wordlist):\n",
    "                    wf, wfvalue = w\n",
    "                    if wf not in self.weights or wfvalue == 0:\n",
    "                        continue\n",
    "                    weights = self.weights[wf]\n",
    "                    for label, weight in weights.items():\n",
    "                        scores[label] += wfvalue * weight\n",
    "            else:\n",
    "                if feat not in self.weights or value == 0:\n",
    "                    continue\n",
    "                weights = self.weights[feat]\n",
    "                for label, weight in weights.items():\n",
    "                    scores[label] += value * weight\n",
    "        return scores\n",
    "    \n",
    "    def get_scored_classes(self, features):\n",
    "        scores = self.get_scores(features)\n",
    "        return sorted(self.classes, key=lambda label: (scores[label], label), reverse=True)\n",
    "    \n",
    "    def predict(self, features):\n",
    "        scores = self.get_scores(features)\n",
    "        return max(self.classes, key=lambda label: (scores[label], label))\n",
    "\n",
    "    def update(self, truth, guess, features):\n",
    "        '''Update the feature weights.'''\n",
    "        def upd_feat(c, f, w, v):\n",
    "            param = (f, c)\n",
    "            self._totals[param] += (self.i - self._tstamps[param]) * w\n",
    "            self._tstamps[param] = self.i\n",
    "            self.weights[f][c] = w + v\n",
    "\n",
    "        self.i += 1\n",
    "        if truth == guess:\n",
    "            return None\n",
    "        for f in features:\n",
    "            if isinstance(features[f], list):\n",
    "                for w in features[f]:\n",
    "                    wf = w[0]\n",
    "                    weights = self.weights.setdefault(wf, {})\n",
    "                    upd_feat(truth, wf, weights.get(truth, 0.0), 1.0)\n",
    "                    upd_feat(guess, wf, weights.get(guess, 0.0), -1.0)\n",
    "            else:\n",
    "                weights = self.weights.setdefault(f, {})\n",
    "                upd_feat(truth, f, weights.get(truth, 0.0), 1.0)\n",
    "                upd_feat(guess, f, weights.get(guess, 0.0), -1.0)\n",
    "        return None\n",
    "\n",
    "    def average_weights(self):\n",
    "        '''Average weights from all iterations.'''\n",
    "        for feat, weights in self.weights.items():\n",
    "            new_feat_weights = {}\n",
    "            for clas, weight in weights.items():\n",
    "                param = (feat, clas)\n",
    "                total = self._totals[param]\n",
    "                total += (self.i - self._tstamps[param]) * weight\n",
    "                averaged = round(total / float(self.i), 3)\n",
    "                if averaged:\n",
    "                    new_feat_weights[clas] = averaged\n",
    "            self.weights[feat] = new_feat_weights\n",
    "        return None\n",
    "\n",
    "    def save(self, path):\n",
    "        '''Save the pickled model weights.'''\n",
    "        return pickle.dump((dict(self.weights), self.classes), open(path, 'wb'))\n",
    "\n",
    "    def load(self, path):\n",
    "        '''Load the pickled model weights.'''\n",
    "        self.weights = pickle.load(open(path))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def pos_chunk(sent, tagger, parser):\n",
    "    \"\"\"\n",
    "    Use tagger and chunk parser to create chunks.\n",
    "    \"\"\"\n",
    "    vps, nps  = [], []\n",
    "    pos_sent = tagger.tag(sent)\n",
    "    parsed_sent = parser.parse(pos_sent)\n",
    "    for s in parsed_sent.subtrees():\n",
    "        if s.label() == 'VP':\n",
    "            vps.append(s.leaves())\n",
    "        elif s.label() == 'NP':\n",
    "            nps.append(s.leaves())\n",
    "    return nps + vps\n",
    "\n",
    "def pos_chunk2(sent, tagger, parser):\n",
    "    vps, nps  = [], []\n",
    "    pos_sent = tagger.tag(sent)\n",
    "    parsed_sent = parser.parse(pos_sent)\n",
    "    for s in parsed_sent.subtrees():\n",
    "        if s.label() == 'VP':\n",
    "            vps.append([w[0] for w in s.leaves()])\n",
    "        elif s.label() == 'NP':\n",
    "            nps.append([w[0] for w in s.leaves()])\n",
    "    return [' '.join(phrase) for phrase in (nps + vps)]\n",
    "\n",
    "# create grammar and initialize chunk parser\n",
    "grammar = r\"\"\"\n",
    "NP: {<DET><ADJ><NOUN>+}\n",
    "    {<DET>*<ADJ><NOUN>+}\n",
    "    {<DET><ADJ>*<NOUN>+}\n",
    "    {<DET><ADJ><NOUN>*<PROPN>*}\n",
    "    {<DET>*<ADJ>*<NOUN>+<ADJ>*<NOUN|PROPN>+}\n",
    "    {<NOUN><NOUN|PROPN>(<CCONJ><NOUN|PROPN>)?}\n",
    "    {<DET>*(<ADJ>|<ADJ><CCONJ><ADJ>)*(<NOUN|PROPN><CCONJ><NOUN|PROPN>|<NOUN|PROPN>)+}\n",
    "VP: {<ADV>*<VERB>+}\n",
    "    {<AUX>+<VERB>+}\n",
    "\"\"\"\n",
    "chunk_parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open cleaned data from saved dictionaries\n",
    "with open('countries.pkl', 'rb') as f:\n",
    "    country_dict = pickle.load(f)\n",
    "    \n",
    "allkeys = []\n",
    "for c in country_dict:\n",
    "    for k in country_dict[c]:\n",
    "        allkeys.append(k)\n",
    "allkeys = set(allkeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Яка загальноприйнята назва Польща</td>\n",
       "      <td>Загальновживана назва</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Яка поширена назва Польща</td>\n",
       "      <td>Загальновживана назва</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Яка загальнопоширена назва Польща</td>\n",
       "      <td>Загальновживана назва</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Яка довга назва Польща</td>\n",
       "      <td>Офіційна назва</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Яка повна назва Польща</td>\n",
       "      <td>Офіційна назва</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Q                      K\n",
       "0  Яка загальноприйнята назва Польща  Загальновживана назва\n",
       "1          Яка поширена назва Польща  Загальновживана назва\n",
       "2  Яка загальнопоширена назва Польща  Загальновживана назва\n",
       "3             Яка довга назва Польща         Офіційна назва\n",
       "4             Яка повна назва Польща         Офіційна назва"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open csv with pairs for training\n",
    "traindf = pd.read_csv('train2.csv')\n",
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_hyphens(sent):\n",
    "    \"\"\"\n",
    "    sent is tokenized with tokenize_uk\n",
    "    \"\"\"\n",
    "    new_sent = []\n",
    "    i = 0\n",
    "    while i < len(sent):\n",
    "        w = sent[i]\n",
    "        if w == '—' or w == '-':\n",
    "            new_sent.pop()\n",
    "            new_word = sent[i-1]+'-'+sent[i+1]\n",
    "            new_sent.append(new_word)\n",
    "            i += 1\n",
    "        else:\n",
    "            new_sent.append(w)\n",
    "        i += 1\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_agree(w_parsed):\n",
    "    \"\"\"\n",
    "    Inflect noun phrase with adjective the right way\n",
    "    \"\"\"\n",
    "    gender = w_parsed.tag.gender\n",
    "    if not gender:\n",
    "        return w_parsed.normal_form\n",
    "    w = w_parsed.inflect({gender, 'nomn'}).word\n",
    "    return w\n",
    "   \n",
    "def get_entity(q_text, lem_dict):\n",
    "    \"\"\"\n",
    "    Look for (capitalized) entities in q_text.\n",
    "    For this specific application pymorphy2 tagging is enough.\n",
    "    \"\"\"\n",
    "    forbidden = ['ВВП', 'HDI', 'ISO', 'ООН', 'UN', 'UTC', \n",
    "                 'Utc-Поправка', 'Utc-Поправка']\n",
    "    words = fix_hyphens(tokenize_words(q_text))\n",
    "    phrase = []\n",
    "    for i, w in enumerate(words[1:]):\n",
    "        if w in forbidden:\n",
    "            continue\n",
    "        if w[0] == w[0].upper():\n",
    "            w_parsed = morph.parse(w.strip(' ?'))[0]\n",
    "            if w_parsed.normal_form not in lem_dict:\n",
    "                continue\n",
    "            if 'ADJF' in w_parsed.tag:\n",
    "                phrase.append(gender_agree(w_parsed).title())\n",
    "                phrase.append(morph.parse\n",
    "                              (words[i+2].strip(' ?'))[0].normal_form)\n",
    "                return ' '.join(phrase).title()\n",
    "            elif 'NOUN' in w_parsed.tag:\n",
    "                return w_parsed.normal_form.title()\n",
    "            elif 'UNKN' in w_parsed.tag:\n",
    "                return w_parsed.normal_form.title()\n",
    "            else:\n",
    "                continue\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_phrase(phrase):\n",
    "    \"\"\"\n",
    "    Also we can stem instead of lemmatizing...\n",
    "    \"\"\"\n",
    "    words = fix_hyphens(tokenize_words(phrase))\n",
    "    if len(words) == 1:\n",
    "        return morph.parse(phrase)[0].normal_form\n",
    "    else:\n",
    "        new_phrase = ''\n",
    "        for w in words:\n",
    "            new_phrase += morph.parse(w)[0].normal_form + ' '\n",
    "        return new_phrase.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_question(q, lem_dict):\n",
    "    ent = get_entity(q, lem_dict)\n",
    "    lem_sent = lemmatize_phrase(q)\n",
    "    lem_ent = lemmatize_phrase(ent)\n",
    "    new_sent = lem_sent.replace(lem_ent, '').replace('  ', ' ')\n",
    "    new_sent = new_sent.replace('який', '')\n",
    "    phrases = pos_chunk2(new_sent, tagger, cp)\n",
    "    return new_sent.strip(), phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = joblib.load('NER_model.pkl')\n",
    "\n",
    "pos_tagger = tagger\n",
    "def get_ner_features(word, prev_word, next_word):\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'word_stem': UkrainianStemmer(word).stem_word(),\n",
    "        'prev_word': prev_word,\n",
    "        'next_word': next_word,\n",
    "        'prev_stem': UkrainianStemmer(prev_word).stem_word(),\n",
    "        'next_stem': UkrainianStemmer(next_word).stem_word(),\n",
    "        'is_uppercase': word.title() == word,\n",
    "        'is_after_punct': prev_word in string.punctuation,\n",
    "        'is_after_uppercase': prev_word.title() == prev_word,\n",
    "        'pos': pos_tagger.tag(' '.join([prev_word, word, next_word]))[1][1]\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def ner_recognize(sent, model):\n",
    "    sent = sent.strip(string.punctuation)\n",
    "    tokens = tokenize_words(sent)\n",
    "    feats = []\n",
    "    for (i, t) in enumerate(tokens):\n",
    "        if i == 0:\n",
    "            prev_word = '.'\n",
    "        else:\n",
    "            prev_word = tokens[i-1]\n",
    "        if i == len(tokens)-1:\n",
    "            next_word = '.'\n",
    "        else:\n",
    "            next_word = tokens[i+1]\n",
    "        feats.append(get_ner_features(t, prev_word, next_word))\n",
    "    labels = model.predict(feats)\n",
    "    return list(zip(tokens, labels))\n",
    "\n",
    "def ner_recognize(sent, model):\n",
    "    tokens = tokenize_words(sent)\n",
    "    feats = []\n",
    "    for (i, t) in enumerate(tokens):\n",
    "        if i == 0:\n",
    "            prev_word = '.'\n",
    "        else:\n",
    "            prev_word = tokens[i-1]\n",
    "        if i == len(tokens)-1:\n",
    "            next_word = '.'\n",
    "        else:\n",
    "            next_word = tokens[i+1]\n",
    "        feats.append(get_ner_features(t, prev_word, next_word))\n",
    "    labels = model.predict(feats)\n",
    "    return list(zip(tokens, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Яка', '-'), ('висота', '-'), ('Гімалаїв', 'LOC')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_recognize('Яка висота Гімалаїв', ner_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Південна Корея', 'Північна Корея', 'Південна Осетія']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_close_matches('Південна Корея', country_dict.keys(), cutoff=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity2(q, ner_model, info_dict, lem_dict):\n",
    "    all_ents = info_dict.keys()\n",
    "    parsed = ner_recognize(q, ner_model)\n",
    "    entities = [e[0] for e in parsed if e[1] == 'LOC']\n",
    "    if not entities:\n",
    "        return get_entity(q, lem_dict)\n",
    "    matches = get_close_matches(entities[0], all_ents)\n",
    "    if not matches:\n",
    "        print(\"Не вдалось знайти географічний об'єкт!\")\n",
    "        print(ent)\n",
    "        return None\n",
    "    return matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionParser():\n",
    "    \n",
    "    def __init__(self, obj_dict, load=True, model_loc = 'qa_model.pkl'):\n",
    "        allkeys = []\n",
    "        for c in obj_dict:\n",
    "            for k in obj_dict[c]:\n",
    "                allkeys.append(k)\n",
    "        allkeys = set(allkeys)\n",
    "        self.qa_model = AveragedPerceptron()\n",
    "        self.classes = allkeys\n",
    "        self.obj_dict = obj_dict\n",
    "        self.lem_dict = [morph.parse(ent.split()[0])[0].normal_form \n",
    "                         for ent in self.obj_dict.keys()]\n",
    "        try:\n",
    "            self.ner_model = joblib.load('NER_model.pkl')\n",
    "        except:\n",
    "            self.ner_model = {}\n",
    "            print('No NER model found!')\n",
    "        if load:\n",
    "            self.load(model_loc)\n",
    "        else:\n",
    "            print('Please train the model for QA.')\n",
    "            \n",
    "    def load(self, loc):\n",
    "        try:\n",
    "            weights, classes = pickle.load(open(loc, 'rb'))\n",
    "        except IOError:\n",
    "            msg = (\"Missing a pickle file for QA model.\")\n",
    "        self.qa_model.weights = weights\n",
    "        self.qa_model.classes = classes\n",
    "        return None\n",
    "    \n",
    "    def get_features(self, q):\n",
    "        \"\"\"\n",
    "        Given question, get features from it.\n",
    "        \"\"\"\n",
    "        sent, phrases = parse_question(q, self.lem_dict)\n",
    "        features = {}\n",
    "        words = fix_hyphens(tokenize_words(sent))\n",
    "        #phrases = ['_'.join([p for p in fix_hyphens(tokenize_words(phrase))]) for phrase in phrases]\n",
    "        for i, w in enumerate(words):\n",
    "            features['word_{i}={w}'.format(i=i, w=w)] = 1\n",
    "        features['words'] = [('w={w}'.format(w=w), 1) for w in words]\n",
    "        bigrams = ['_'.join(b) for b in nltk.bigrams(words)]\n",
    "        features['bigrams'] = [('bg={bg}'.format(bg=bg), 1) for bg in bigrams]\n",
    "        n = 3\n",
    "        char_trigrams = [sent[i:i+n] for i in range(len(sent)-n+1)]\n",
    "        features['trigrams'] = [('t={t}'.format(t=t), 1) for t in char_trigrams]\n",
    "        #features['phrases'] = []\n",
    "        #for ph in phrases:\n",
    "        #    if not ph in bigrams:\n",
    "        #        features['phrases'].append(('ph={ph}'.format(ph=ph), 1))\n",
    "        return features\n",
    "\n",
    "    def train(self, train_df, n_iter=5, save=True, loc='qa_model.pkl'):\n",
    "        \"\"\"\n",
    "        obj_dict is currently country_dict.\n",
    "        train_df contains columns Q and A\n",
    "        \"\"\"\n",
    "        self.qa_model.classes = self.classes\n",
    "        for iteration in range(n_iter):\n",
    "            print('Training iteration number', iteration+1)\n",
    "            train_df = train_df.sample(len(train_df))\n",
    "            for i, row in train_df.iterrows():\n",
    "                q = row['Q']\n",
    "                k = row['K']\n",
    "                ent = get_entity2(q, self.ner_model, self.obj_dict,\n",
    "                                  self.lem_dict)\n",
    "                if not ent:\n",
    "                    print('Cannot find an entity for', q)\n",
    "                    continue\n",
    "                true_keys = []\n",
    "                if ent not in self.obj_dict:\n",
    "                    print('Cannot find an entity in a dictionary for', q)\n",
    "                    print(ent)\n",
    "                feats = self.get_features(q)\n",
    "                guess = self.qa_model.predict(feats)\n",
    "                self.qa_model.update(k, guess, feats)\n",
    "        self.qa_model.average_weights()\n",
    "        if save:\n",
    "            self.qa_model.save(loc)\n",
    "        return None\n",
    "    \n",
    "    def find_answer(self, q):\n",
    "        ent = get_entity2(q, self.ner_model, self.obj_dict,\n",
    "                          self.lem_dict)\n",
    "        if not ent:\n",
    "            return 'Відповідь не знайшлась.'\n",
    "        feats = self.get_features(q)\n",
    "        all_classes = self.qa_model.get_scores(feats)\n",
    "        pred_classes = self.qa_model.get_scored_classes(feats)\n",
    "        for cl in pred_classes:\n",
    "            if cl in self.obj_dict[ent] and cl in all_classes:\n",
    "                a = self.obj_dict[ent][cl]\n",
    "                answer = '{cl} для {ent} - {a}'.format(cl=cl, ent=ent, a=a)\n",
    "                return answer\n",
    "        return 'Відповідь не знайшлась.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(train_df):\n",
    "    def get_features(self, q):\n",
    "        \"\"\"\n",
    "        Given question, get features from it.\n",
    "        \"\"\"\n",
    "        sent, phrases = parse_question(q, lem_dict)\n",
    "        features = {}\n",
    "        words = fix_hyphens(tokenize_words(sent))\n",
    "        for i, w in enumerate(words):\n",
    "            features['word_{i}={w}'.format(i=i, w=w)] = 1\n",
    "        features['words'] = [('w={w}'.format(w=w), 1) for w in words]\n",
    "        bigrams = ['_'.join(b) for b in nltk.bigrams(words)]\n",
    "        features['bigrams'] = [('bg={bg}'.format(bg=bg), 1) for bg in bigrams]\n",
    "        n = 3\n",
    "        char_trigrams = [sent[i:i+n] for i in range(len(sent)-n+1)]\n",
    "        features['trigrams'] = [('t={t}'.format(t=t), 1) for t in char_trigrams]\n",
    "        return features\n",
    "    \n",
    "    features, labels = [], []\n",
    "    for i, row in train_df.iterrows():\n",
    "        q = row['Q']\n",
    "        k = row['K']\n",
    "        ent = get_entity2(q, self.ner_model, self.obj_dict,\n",
    "                          self.lem_dict)\n",
    "        if not ent:\n",
    "            print('Cannot find an entity for', q)\n",
    "            continue\n",
    "        feats = get_features(q)\n",
    "        features.append(feats)\n",
    "        labels.append(k)\n",
    "    vec = DictVectorizer()\n",
    "    train_feats = vec.fit_transform(features)\n",
    "    clf = LogisticRegression(penalty='l1')\n",
    "    clf.fit(train_feats, labels)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-09ef11158b37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraindf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-79-2e6e2df4b1ac>\u001b[0m in \u001b[0;36mtrain2\u001b[0;34m(train_df)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Q'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'K'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         ent = get_entity2(q, self.ner_model, self.obj_dict,\n\u001b[0m\u001b[1;32m      7\u001b[0m                           self.lem_dict)\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "lr_model = train2(traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Який', '-'),\n",
       " ('Алжир', 'LOC'),\n",
       " ('за', '-'),\n",
       " ('населенням', '-'),\n",
       " ('у', '-'),\n",
       " ('світі', '-'),\n",
       " ('?', '-')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_recognize('Який Алжир за населенням у світі?', ner_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please train the model for QA.\n",
      "Training iteration number 1\n",
      "Training iteration number 2\n",
      "Training iteration number 3\n",
      "Training iteration number 4\n",
      "Training iteration number 5\n",
      "Training iteration number 6\n",
      "Training iteration number 7\n",
      "Training iteration number 8\n",
      "Training iteration number 9\n",
      "Training iteration number 10\n",
      "Training iteration number 11\n",
      "Training iteration number 12\n",
      "Training iteration number 13\n",
      "Training iteration number 14\n",
      "Training iteration number 15\n",
      "Training iteration number 16\n",
      "Training iteration number 17\n",
      "Training iteration number 18\n",
      "Training iteration number 19\n",
      "Training iteration number 20\n"
     ]
    }
   ],
   "source": [
    "qp = QuestionParser(country_dict, load=False)\n",
    "qp.train(traindf, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Столиця для Екваторіальна Гвінея - Малабо'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qp = QuestionParser(country_dict)\n",
    "qp.find_answer('яка столиця Екваторіальної Гвінеї')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'Столиця': 9.912000000000003,\n",
       "             'Валюта': -4.952,\n",
       "             'Густота населення': 1.966,\n",
       "             'Офіційна назва': -0.999,\n",
       "             'ВВП (ПКС)': -0.995,\n",
       "             'Найбільше місто': -1.971,\n",
       "             'Ключові дати в історії': -0.385,\n",
       "             'Місце за густотою населення': 1.964,\n",
       "             'Місце за населенням': -1.971,\n",
       "             'Місце за площею': -0.962,\n",
       "             'Ключові події і дати в історії': 0.382,\n",
       "             'Офіційні мови': -0.992,\n",
       "             'Посади лідерів': -0.996})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = qp.get_features('яка столиця Екваторіальної Гвінеї')\n",
    "qp.qa_model.get_scores(feats)\n",
    "#feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = 'Яка столиця Польщі'\n",
    "get_entity2(q, qp.ner_model, qp.obj_dict,\n",
    "                          qp.lem_dict)\n",
    "feats = qp.get_features(q)\n",
    "all_classes = qp.qa_model.get_scores(feats)\n",
    "pred_classes = qp.qa_model.get_scored_classes(feats)\n",
    "#for cl in pred_classes:\n",
    "#    if cl in self.obj_dict[ent] and cl in all_classes:\n",
    "#        a = self.obj_dict[ent][cl]\n",
    "#        answer = '{cl} для {ent} - {a}'.format(cl=cl, ent=ent, a=a)\n",
    "#        return answer\n",
    "qp.qa_model.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_questions.txt', 'r') as f:\n",
    "    tq = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "21/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "яка площа Мексики\n",
      "Площа для Мексика - 1972550\n",
      "---\n",
      "---\n",
      "яка площа території Португалії\n",
      "Площа для Португалія - нема інформації в базі\n",
      "---\n",
      "---\n",
      "яка територія Гвінеї\n",
      "Площа для Гвінея - 245.857\n",
      "---\n",
      "---\n",
      "який розмір Гвінеї\n",
      "Площа для Гвінея - 245.857\n",
      "---\n",
      "---\n",
      "яка столиця Мексики\n",
      "Столиця для Мексика - Мехіко\n",
      "---\n",
      "---\n",
      "яке місто є столиця Мексики\n",
      "Найбільше місто для Мексика - Мехіко\n",
      "---\n",
      "---\n",
      "яка офіційна мова Австралії\n",
      "Офіційні мови для Австралія - Англійська мова (англійська1)\n",
      "---\n",
      "---\n",
      "яка мова визнана в Мексиці офіційною?\n",
      "Офіційні мови для Мексика - іспанська мова\n",
      "---\n",
      "---\n",
      "яка форма правління Мексики\n",
      "Форма правління для Мексика - Федеративна республіка\n",
      "---\n",
      "---\n",
      "хто є президентом України\n",
      "Імена лідерів для Україна - Порошенко Петро Олексійович, Гройсман Володимир Борисович, Парубій Андрій Володимирович\n",
      "---\n",
      "---\n",
      "хто польский президент?\n",
      "Відповідь не знайшлась.\n",
      "---\n",
      "---\n",
      "коли відбулося хрещення Гвінеї\n",
      "Ключові дати в історії для Гвінея - 2 жовтня 1958\n",
      "---\n",
      "---\n",
      "у якому році відбулось хрещення Гвінеї\n",
      "UTC-поправка для Гвінея - нема інформації в базі\n",
      "---\n",
      "---\n",
      "яка чисельність населення Гвінеї\n",
      "Населення для Гвінея - 9.402.000\n",
      "---\n",
      "---\n",
      "скільки людей проживає в України\n",
      "Густота населення для Україна -   осіб\n",
      "---\n",
      "---\n",
      "яка густота населення Австралії\n",
      "Густота населення для Австралія - 2.8\n",
      "---\n",
      "---\n",
      "який ВВП на душу населення у Гвінеї\n",
      "ВВП (ПКС) на душу населення для Гвінея - $2,035 \n",
      "---\n",
      "---\n",
      "який повний ВВП Ботсвани\n",
      "ВВП (ПКС) для Ботсвана - $18.72 млрд.\n",
      "---\n",
      "---\n",
      "яка валюта Ботсвани\n",
      "Валюта для Ботсвана - Ботсванська пула (Пула)\n",
      "---\n",
      "---\n",
      "як називається валюта, яку використовують у Португалії\n",
      "Валюта для Португалія - Євро\n",
      "---\n",
      "---\n",
      "який часовий пояс Мексики\n",
      "Часовий пояс для Мексика - нема інформації в базі\n",
      "---\n",
      "---\n",
      "який домен України\n",
      "Інтернет-домен для Україна - .ua, .укр\n",
      "---\n",
      "---\n",
      "який телефонний код Ботсвани\n",
      "Телефонний код для Ботсвана - 267\n",
      "---\n",
      "---\n",
      "які офіційні мови Гвінеї\n",
      "Офіційні мови для Гвінея - Французька мова (Французька)\n",
      "---\n",
      "---\n",
      "чи належить німецька до офіційних мов Австралії\n",
      "Назва державною мовою для Австралія - нема інформації в базі\n",
      "---\n",
      "---\n",
      "столицею якої країни є Тегусігальпа\n",
      "Не вдалось знайти географічний об'єкт!\n",
      "Тегусігальпа\n",
      "Відповідь не знайшлась.\n",
      "---\n",
      "---\n",
      "в якому регіоні розміщена Вінниця\n",
      "Не вдалось знайти географічний об'єкт!\n",
      "Вінниця\n",
      "Відповідь не знайшлась.\n",
      "---\n",
      "---\n",
      "який девіз Вінниці\n",
      "Не вдалось знайти географічний об'єкт!\n",
      "Вінниці\n",
      "Відповідь не знайшлась.\n",
      "---\n",
      "---\n",
      "коли було засновано Вінницю\n",
      "Не вдалось знайти географічний об'єкт!\n",
      "Вінницю\n",
      "Відповідь не знайшлась.\n",
      "---\n",
      "---\n",
      "у якому столітті було засновано Париж\n",
      "Не вдалось знайти географічний об'єкт!\n",
      "Париж\n",
      "Відповідь не знайшлась.\n",
      "---\n",
      "---\n",
      "яке населення Сакраменто\n",
      "Не вдалось знайти географічний об'єкт!\n",
      "Сакраменто\n",
      "Відповідь не знайшлась.\n",
      "---\n",
      "---\n",
      "скільки людей живе у Пекіні\n",
      "Не вдалось знайти географічний об'єкт!\n",
      "Пекіні\n",
      "Відповідь не знайшлась.\n",
      "---\n",
      "---\n",
      "скільки людей мешкає у Римі\n",
      "Не вдалось знайти географічний об'єкт!\n",
      "Римі\n",
      "Відповідь не знайшлась.\n",
      "---\n",
      "---\n",
      "скільки людей проживає в агломерації Пекіну?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-635cd85d58d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-6e137ef8ae1e>\u001b[0m in \u001b[0;36mfind_answer\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m'Відповідь не знайшлась.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mall_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mpred_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scored_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-6e137ef8ae1e>\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \"\"\"\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphrases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlem_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix_hyphens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f53d91da8610>\u001b[0m in \u001b[0;36mparse_question\u001b[0;34m(q, lem_dict)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0ment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_entity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlem_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlem_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlem_ent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mnew_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlem_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlem_ent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnew_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'який'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-218598042738>\u001b[0m in \u001b[0;36mlemmatize_phrase\u001b[0;34m(phrase)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix_hyphens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnew_phrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Bin/anaconda3/lib/python3.6/site-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0mword_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_terminal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_units\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "for q_text in tq:\n",
    "    print('---')\n",
    "    print(q_text)\n",
    "    print(qp.find_answer(q_text))\n",
    "    print('---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
